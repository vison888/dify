# 15-定时任务层

## 概述

定时任务层（`schedule/`）负责管理和执行系统中的定时任务、后台作业和系统维护工作。该层基于Celery任务队列系统，提供数据清理、缓存管理、监控告警等功能，确保系统的稳定运行和数据一致性。

## 目录结构

```
schedule/
├── clean_messages.py                    # 消息清理任务
├── clean_embedding_cache_task.py        # 嵌入缓存清理任务
├── clean_unused_datasets_task.py        # 未使用数据集清理任务
├── queue_monitor_task.py                # 队列监控任务
├── check_upgradable_plugin_task.py      # 插件升级检查任务
├── create_tidb_serverless_task.py       # TiDB Serverless创建任务
├── update_tidb_serverless_status_task.py # TiDB Serverless状态更新任务
└── mail_clean_document_notify_task.py   # 文档清理邮件通知任务
```

## 核心任务详解

### 1. `clean_messages.py` - 消息清理任务

**职责**: 定期清理过期的消息数据，特别是沙盒环境的消息。

**核心功能**:
```python
@app.celery.task(queue="dataset")
def clean_messages():
    """清理过期消息任务"""
    click.echo(click.style("Start clean messages.", fg="green"))
    start_at = time.perf_counter()
    
    # 计算清理时间点
    plan_sandbox_clean_message_day = datetime.datetime.now() - datetime.timedelta(
        days=dify_config.PLAN_SANDBOX_CLEAN_MESSAGE_DAY_SETTING
    )
    
    while True:
        try:
            # 批量查询过期消息
            messages = (
                db.session.query(Message)
                .filter(Message.created_at < plan_sandbox_clean_message_day)
                .order_by(Message.created_at.desc())
                .limit(100)
                .all()
            )
        except NotFound:
            break
            
        if not messages:
            break
            
        for message in messages:
            plan_sandbox_clean_message_day = message.created_at
            app = db.session.query(App).filter_by(id=message.app_id).first()
            
            if not app:
                _logger.warning(
                    "Expected App record to exist, but none was found, app_id=%s, message_id=%s",
                    message.app_id,
                    message.id,
                )
                continue
                
            # 检查租户计划
            features_cache_key = f"features:{app.tenant_id}"
            plan_cache = redis_client.get(features_cache_key)
            
            if plan_cache is None:
                features = FeatureService.get_features(app.tenant_id)
                redis_client.setex(features_cache_key, 600, features.billing.subscription.plan)
                plan = features.billing.subscription.plan
            else:
                plan = plan_cache.decode()
                
            # 清理沙盒环境的消息
            if plan == "sandbox":
                # 清理相关消息数据
                db.session.query(MessageFeedback).filter(MessageFeedback.message_id == message.id).delete(
                    synchronize_session=False
                )
                db.session.query(MessageAnnotation).filter(MessageAnnotation.message_id == message.id).delete(
                    synchronize_session=False
                )
                db.session.query(MessageChain).filter(MessageChain.message_id == message.id).delete(
                    synchronize_session=False
                )
                db.session.query(MessageAgentThought).filter(MessageAgentThought.message_id == message.id).delete(
                    synchronize_session=False
                )
                db.session.query(MessageFile).filter(MessageFile.message_id == message.id).delete(
                    synchronize_session=False
                )
                db.session.query(SavedMessage).filter(SavedMessage.message_id == message.id).delete(
                    synchronize_session=False
                )
                db.session.query(Message).filter(Message.id == message.id).delete()
                db.session.commit()
    
    end_at = time.perf_counter()
    click.echo(click.style("Cleaned messages from db success latency: {}".format(end_at - start_at), fg="green"))
```

**设计特点**:
- 批量处理，避免内存溢出
- 租户计划检查，只清理沙盒环境
- 关联数据清理，确保数据一致性
- 性能监控和日志记录

### 2. `clean_embedding_cache_task.py` - 嵌入缓存清理任务

**职责**: 清理过期的嵌入向量缓存数据。

**核心功能**:
```python
@app.celery.task(queue="dataset")
def clean_embedding_cache_task():
    """清理嵌入缓存任务"""
    click.echo(click.style("Start clean embedding cache.", fg="green"))
    clean_days = int(dify_config.PLAN_SANDBOX_CLEAN_DAY_SETTING)
    start_at = time.perf_counter()
    
    # 计算清理时间点
    thirty_days_ago = datetime.datetime.now() - datetime.timedelta(days=clean_days)
    
    while True:
        try:
            # 批量查询过期嵌入向量
            embedding_ids = (
                db.session.query(Embedding.id)
                .filter(Embedding.created_at < thirty_days_ago)
                .order_by(Embedding.created_at.desc())
                .limit(100)
                .all()
            )
            embedding_ids = [embedding_id[0] for embedding_id in embedding_ids]
        except NotFound:
            break
            
        if embedding_ids:
            # 批量删除过期嵌入向量
            for embedding_id in embedding_ids:
                db.session.execute(
                    text("DELETE FROM embeddings WHERE id = :embedding_id"), 
                    {"embedding_id": embedding_id}
                )
            db.session.commit()
        else:
            break
    
    end_at = time.perf_counter()
    click.echo(click.style("Cleaned embedding cache from db success latency: {}".format(end_at - start_at), fg="green"))
```

**设计特点**:
- 使用原生SQL提高删除性能
- 批量处理，减少数据库压力
- 可配置的清理时间间隔

### 3. `queue_monitor_task.py` - 队列监控任务

**职责**: 监控Celery队列状态，当队列积压超过阈值时发送告警邮件。

**核心功能**:
```python
@app.celery.task(queue="monitor")
def queue_monitor_task():
    """队列监控任务"""
    queue_name = "dataset"
    threshold = dify_config.QUEUE_MONITOR_THRESHOLD

    try:
        # 获取队列长度
        queue_length = celery_redis.llen(f"{queue_name}")
        logging.info(click.style(f"Start monitor {queue_name}", fg="green"))
        logging.info(click.style(f"Queue length: {queue_length}", fg="green"))

        # 检查是否超过阈值
        if queue_length >= threshold:
            warning_msg = f"Queue {queue_name} task count exceeded the limit.: {queue_length}/{threshold}"
            logging.warning(click.style(warning_msg, fg="red"))
            
            # 发送告警邮件
            alter_emails = dify_config.QUEUE_MONITOR_ALERT_EMAILS
            if alter_emails:
                to_list = alter_emails.split(",")
                email_service = get_email_i18n_service()
                
                for to in to_list:
                    try:
                        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        email_service.send_email(
                            email_type=EmailType.QUEUE_MONITOR_ALERT,
                            language_code="en-US",
                            to=to,
                            template_context={
                                "queue_name": queue_name,
                                "queue_length": queue_length,
                                "threshold": threshold,
                                "alert_time": current_time,
                            },
                        )
                    except Exception as e:
                        logging.exception(click.style("Exception occurred during sending email", fg="red"))

    except Exception as e:
        logging.exception(click.style("Exception occurred during queue monitoring", fg="red"))
    finally:
        if db.session.is_active:
            db.session.close()
```

**设计特点**:
- 实时监控队列状态
- 可配置的告警阈值
- 邮件告警通知
- 异常处理和资源清理

### 4. `check_upgradable_plugin_task.py` - 插件升级检查任务

**职责**: 定期检查插件是否有可用更新。

**核心功能**:
```python
@app.celery.task(queue="dataset")
def check_upgradable_plugin_task():
    """检查可升级插件任务"""
    try:
        # 获取所有已安装插件
        installed_plugins = get_installed_plugins()
        
        for plugin in installed_plugins:
            # 检查是否有新版本
            latest_version = check_plugin_latest_version(plugin.name)
            
            if latest_version and latest_version > plugin.version:
                # 记录可升级的插件
                mark_plugin_upgradable(plugin.id, latest_version)
                
    except Exception as e:
        logging.exception("Failed to check upgradable plugins")
```

### 5. `clean_unused_datasets_task.py` - 未使用数据集清理任务

**职责**: 清理长时间未使用的数据集和相关资源。

**核心功能**:
```python
@app.celery.task(queue="dataset")
def clean_unused_datasets_task():
    """清理未使用数据集任务"""
    # 获取未使用的数据集
    unused_datasets = get_unused_datasets()
    
    for dataset in unused_datasets:
        try:
            # 清理数据集文件
            clean_dataset_files(dataset)
            
            # 清理向量数据库
            clean_vector_database(dataset)
            
            # 删除数据集记录
            delete_dataset_record(dataset)
            
        except Exception as e:
            logging.exception(f"Failed to clean dataset {dataset.id}")
```

## 定时任务配置

### 1. Celery Beat调度配置

```python
# 在ext_celery.py中配置定时任务
celery_app.conf.beat_schedule = {
    'clean-expired-messages': {
        'task': 'schedule.clean_messages',
        'schedule': crontab(hour=2, minute=0),  # 每天凌晨2点执行
    },
    'clean-embedding-cache': {
        'task': 'schedule.clean_embedding_cache_task',
        'schedule': crontab(hour=3, minute=0),  # 每天凌晨3点执行
    },
    'queue-monitor': {
        'task': 'schedule.queue_monitor_task',
        'schedule': crontab(minute='*/5'),  # 每5分钟执行
    },
    'check-upgradable-plugins': {
        'task': 'schedule.check_upgradable_plugin_task',
        'schedule': crontab(hour=4, minute=0),  # 每天凌晨4点执行
    },
    'clean-unused-datasets': {
        'task': 'schedule.clean_unused_datasets_task',
        'schedule': crontab(hour=5, minute=0),  # 每天凌晨5点执行
    },
}
```

### 2. 任务队列配置

```python
# 不同任务使用不同的队列
QUEUE_CONFIG = {
    "dataset": {
        "concurrency": 4,  # 并发工作进程数
        "max_tasks_per_child": 1000,  # 每个工作进程最大任务数
    },
    "monitor": {
        "concurrency": 1,  # 监控任务使用单进程
        "max_tasks_per_child": 100,
    },
    "default": {
        "concurrency": 2,
        "max_tasks_per_child": 500,
    },
}
```

## 任务设计原则

### 1. 任务分类

**数据清理任务**:
- 消息清理
- 嵌入缓存清理
- 未使用数据集清理

**监控任务**:
- 队列监控
- 系统健康检查
- 性能监控

**维护任务**:
- 插件升级检查
- 数据库优化
- 日志清理

### 2. 设计特点

**幂等性**:
- 任务可以重复执行而不产生副作用
- 支持任务重试机制

**可配置性**:
- 通过配置文件控制任务行为
- 支持环境特定的配置

**监控和告警**:
- 任务执行状态监控
- 异常情况告警通知
- 性能指标收集

## 使用示例

### 1. 手动执行任务

```python
from schedule.clean_messages import clean_messages
from schedule.queue_monitor_task import queue_monitor_task

# 手动执行消息清理
clean_messages.delay()

# 手动执行队列监控
queue_monitor_task.delay()
```

### 2. 任务状态监控

```python
from celery.result import AsyncResult

# 提交任务
task = clean_messages.delay()

# 检查任务状态
result = AsyncResult(task.id)
print(f"Task status: {result.status}")

# 获取任务结果
if result.ready():
    print(f"Task result: {result.get()}")
```

### 3. 任务配置管理

```python
# 动态修改任务调度
from celery.schedules import crontab

# 添加新的定时任务
celery_app.conf.beat_schedule['new-task'] = {
    'task': 'schedule.new_task',
    'schedule': crontab(minute='*/10'),  # 每10分钟执行
}

# 移除定时任务
del celery_app.conf.beat_schedule['old-task']
```

## 最佳实践

### 1. 任务设计原则

**单一职责**:
- 每个任务只负责一个特定功能
- 避免任务间的相互依赖

**错误处理**:
- 提供详细的错误日志
- 支持任务重试机制
- 优雅处理异常情况

**性能优化**:
- 批量处理数据
- 使用适当的队列配置
- 避免长时间运行的任务

### 2. 监控和告警

**任务监控**:
```python
def monitor_task_execution(task_name: str):
    """监控任务执行"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                execution_time = time.time() - start_time
                logging.info(f"Task {task_name} completed in {execution_time:.2f}s")
                return result
            except Exception as e:
                execution_time = time.time() - start_time
                logging.error(f"Task {task_name} failed after {execution_time:.2f}s: {e}")
                raise
        return wrapper
    return decorator
```

**告警配置**:
```python
# 配置告警阈值
ALERT_CONFIG = {
    "queue_length_threshold": 1000,
    "task_execution_time_threshold": 300,  # 5分钟
    "error_rate_threshold": 0.1,  # 10%
}
```

### 3. 测试支持

```python
class MockTaskExecutor:
    """Mock任务执行器"""
    
    def __init__(self):
        self.executed_tasks = []
    
    def execute_task(self, task_name: str, *args, **kwargs):
        """执行任务"""
        self.executed_tasks.append({
            "name": task_name,
            "args": args,
            "kwargs": kwargs,
            "timestamp": datetime.now()
        })
    
    def get_executed_tasks(self):
        """获取已执行的任务"""
        return self.executed_tasks
```

## 扩展指南

### 添加新定时任务

1. **创建任务文件**:
```python
# schedule/new_task.py
import logging
import click

import app
from extensions.ext_database import db

@app.celery.task(queue="dataset")
def new_task():
    """新定时任务"""
    click.echo(click.style("Start new task.", fg="green"))
    
    try:
        # 任务逻辑
        process_data()
        
        click.echo(click.style("New task completed successfully.", fg="green"))
    except Exception as e:
        logging.exception("New task failed")
        raise
```

2. **注册到调度器**:
```python
# 在ext_celery.py中添加
celery_app.conf.beat_schedule['new-task'] = {
    'task': 'schedule.new_task',
    'schedule': crontab(hour=6, minute=0),  # 每天凌晨6点执行
}
```

3. **添加配置**:
```python
# 在配置文件中添加相关配置
class Config:
    NEW_TASK_ENABLED = True
    NEW_TASK_BATCH_SIZE = 100
```

### 自定义任务队列

```python
class CustomTaskQueue:
    """自定义任务队列"""
    
    def __init__(self, queue_name: str, concurrency: int = 1):
        self.queue_name = queue_name
        self.concurrency = concurrency
        self.tasks = []
    
    def add_task(self, task_func, *args, **kwargs):
        """添加任务到队列"""
        self.tasks.append({
            "func": task_func,
            "args": args,
            "kwargs": kwargs
        })
    
    def execute_tasks(self):
        """执行队列中的任务"""
        for task in self.tasks:
            try:
                task["func"](*task["args"], **task["kwargs"])
            except Exception as e:
                logging.exception(f"Task execution failed: {e}")
```

## 故障排除

### 常见问题

1. **任务执行失败**
   - 检查任务日志
   - 验证数据库连接
   - 确认配置正确性

2. **队列积压**
   - 增加工作进程数
   - 优化任务执行时间
   - 检查任务依赖关系

3. **内存泄漏**
   - 定期重启工作进程
   - 检查任务资源清理
   - 监控内存使用情况

### 调试技巧

```python
import logging

logger = logging.getLogger(__name__)

def debug_task_execution(task_name: str):
    """调试任务执行"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            logger.debug(f"开始执行任务: {task_name}")
            logger.debug(f"任务参数: {args}, {kwargs}")
            
            try:
                result = func(*args, **kwargs)
                logger.debug(f"任务 {task_name} 执行成功")
                return result
            except Exception as e:
                logger.error(f"任务 {task_name} 执行失败: {e}")
                raise
        return wrapper
    return decorator
```

---

*定时任务层为Dify提供了可靠的后台任务执行机制，确保系统的稳定运行和数据维护，支持系统的自动化和监控需求。* 